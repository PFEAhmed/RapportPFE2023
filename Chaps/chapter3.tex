%\documentclass{report}
%\input{includePackagesChaps.tex}



%\begin{document}



\chapter{Raspberry pi 4 and ESP32 Cam Integration for Logo Detection}
\section{Introduction}
This chapter is a step-by-step guide on how to train and use the YOLOv5 algorithm for logo detection. The process of logo detection involves identifying logos in images and videos, which can be challenging due to various factors such as logo occlusion, varying sizes and orientations, and the presence of similar objects in the background.

To address these challenges, the chapter walks through the various steps involved in building a logo detection model using the YOLOv5s algorithm, including data gathering and preprocessing, model training, validation, and testing. The YOLOv5s algorithm is a popular object detection algorithm that is widely used due to its high accuracy and fast inference time.

The chapter also covers various techniques for data preprocessing, including image labeling, data augmentation, and data splitting. These techniques help to improve the accuracy of the trained mode which concludes by demonstrating how to use the trained YOLOv5s model to detect logos in new, unseen images .

This chapter coclude by diving into the steps needed to build a relay  module for the comuniation between the raspberry pi and the automaton.

 Overall, this chapter provides a comprehensive guide to building an accurate and efficient logo detection system using the YOLOv5s algorithm.
\section{Specification of requirements}
In this section, we introduce the different actors as well as the functional and non-functional requirements.
\subsection{actors identification }
\subsection{Description of functional requirements}
\begin{itemize}
\item The ESP32-CAM board should be able to capture a live stream of pictures or videos and make them available through its built-in server.
\item The raspberry pi should be able to connect to the server on the ESP32-CAM board and retrieve the images or videos.
\item The raspberry pi should be able to process the images to determine if a specific logo is present or not.
\item The raspberry pi should be able to process the images to determine if a specific logo is flipped on the x axis.
\item The raspberry pi should be able to process the images to determine if a specific logo is in the first or the second half of the product.
\item The raspberry pi should be able to provide some form of feedback or output indicating whether the logo is present and if it is flipped or not.
\end{itemize}
\subsection{Description of non-functional requirements}
The requirements do not stop at the functional level but tend towards requirements that contribute to better quality of the application. The most important ones are:
\begin{itemize}
\item \textbf{Reliability:} The system should be able to consistently capture and process images accurately.
\item \textbf{Performance:} The system should be able to process images quickly and without noticeable lag or delay.
\item \textbf{Security:} The system should have appropriate security measures in place to prevent unauthorized access to the servers and data.
\item \textbf{Scalability:} The system should be able to handle multiple simultaneous connections and requests from clients without compromising its performance.
\item \textbf{Maintainability:} The system should be easy to maintain and update, with clear and well-documented code and configuration.
\item \textbf{Compatibility:} The system should be compatible with a wide range of devices and platforms.
\item \textbf{Usability:} The system should be easy to use and understand for both technical and non-technical users.
\end{itemize}

\section{SysMl}

\FloatBarrier
\begin{figure}[h]

         \centering
        \includegraphics[width=1\textwidth]{UseCase Diagram2}
   
        \caption{Use case diagram version 1}
        \label{Use case diagram version 1}

    \end{figure}


\FloatBarrier

\begin{figure}[h]

         \centering
        \includegraphics[width=1\textwidth]{Activity Diagram2}
   
        \caption{Activity diagram version 1}
        \label{Activity diagram version 1}

    \end{figure}


\FloatBarrier


\begin{figure}[h]

         \centering
        \includegraphics[width=16cm, height=10cm]{Requirement Diagram2}
   
        \caption{Requirement diagram version 1}
        \label{Requirement diagram version 1}

    \end{figure}



\FloatBarrier
\begin{figure}[h]

         \centering
        \includegraphics[width=1\textwidth]{Block Definition Diagram2}
   
        \caption{Block Definition Diagram version 1}
        \label{Block Definition Diagram version 1}

    \end{figure}


\FloatBarrier

\section{Project component}
\subsection{Hardware and Software environment}
For a comprehensive understanding of the hardware and software environment components used in this study, I recommend referring to the annex chapter \ref{Annex} of this report. The annex contains detailed information on the system specifications and configurations, including hardware components such as ESP32-cam, as well as the software components such as  application programs. By referring to the annex, you can gain a deeper insight into the technical details of the system, which can be helpful in evaluating the validity and reliability of the study results.






\section{Workflow}
\subsubsection{Setting up the rapsberry pi}
\begin{itemize}
\item{ setting up the OS }
\begin{itemize}

\item 

\begin{minipage}{0.5\textwidth}
download the Raspberry Pi Imager from the official Raspberry Pi website.
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\hspace*{0.3in} \includegraphics[width=\textwidth]{1i}
\end{minipage}
\vspace{0.5in}

\item 
\begin{minipage}{0.5\textwidth}
choose the raspberry pi operating system (64-bit)
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\includegraphics[width=\textwidth]{3i}
\end{minipage}
\vspace{0.5in}
\item 
\begin{minipage}{0.5\textwidth}
Configure the parameters of the operating system: set a hostname,enable SSH,set username and password,configure wireless lan and set local settings
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\includegraphics[width=\textwidth]{8i}
\end{minipage}
\vspace{0.5in}
\item 
\begin{minipage}{0.5\textwidth}
Select the SD card you want to use for the installation.
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\includegraphics[width=\textwidth]{5i}
\end{minipage}
\vspace{0.5in}
\item 
\begin{minipage}{0.5\textwidth}
Click on the "Write" button to start the installation process. This will take several minutes to complete.
\end{minipage}
\hfill
\begin{minipage}{0.6\textwidth}
\includegraphics[width=\textwidth]{9i}
\end{minipage}
\end{itemize}


\item{setting up the VNC}
\begin{itemize}
\item{Establishing connection with puttty}
\FloatBarrier
\begin{figure}[h]

       \centering

        \includegraphics[width=0.5\textwidth]{1p}
   
        \caption{putty interface}
        \label{fig:putty interface}

    \end{figure}

\FloatBarrier
\end{itemize}
\begin{itemize}
\item In the "Host Name (or IP address)" field, enter the IP address of your Raspberry Pi.
\item In the "Port" field, enter "22". This is the default SSH port for Raspberry Pi.
\item Under "Connection type", select "SSH".
\item Click the "Open" button to start the SSH connection.
\end{itemize}
\item{Updating the system}
\FloatBarrier
\begin{figure}[h]

       \centering

        \includegraphics[width=0.5\textwidth]{updatefile}
   
        \caption{Update.sh file}
        \label{fig:update.sh}

    \end{figure}

\FloatBarrier
I have created \textbf{update.sh} that contain these three commands:
\begin{itemize}
\item\commandbox*{sudo apt-get update} : This command updates the list of available software packages and their versions from the repositories defined in the package manager sources list.
\item \commandbox*{sudo apt-get upgrade}: This command upgrades the installed packages on the system to their latest versions.
\item  \commandbox*{sudo reboot}: This command restarts the system after the update and upgrade processes have completed.
\end{itemize}
To execute the update.sh just run this command:
\commandbox*{sh update.sh}
This table represent the total time taking by this command:\\
\FloatBarrier
\begin{table}[h]

\centering
\tcbox[width=\linewidth,
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title = Execution time of sh update.sh]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



\begin{tabular}{|c|c|}
  \hline
        \textbf{Metric} & \textbf{Value} \\
        \hline

        real   & 9.631 s \\
        \hline
        user & 3.065 s \\
        \hline
        sys & 1.097 s \\
        \hline
\end{tabular}}
    \addcontentsline{lot}{table}{Execution time of sh update.sh}

\label{tab:time}
\end{table}

\FloatBarrier
%\begin{table}[h]
   % \centering
    %\begin{tabular}{|c|c|}
       % \hline
        %\textbf{Metric} & \textbf{Value} \\
        %\hline
        %real & 9.631 s \\
        %\hline
        %user & 3.065 s \\
        %\hline
        %sys & 1.097 s \\
        %\hline
    %\end{tabular}
    %\caption{Execution time of sh update.sh}
    %\label{tab:time}
%\end{table}

\item{Activating VNC server}\\
Enter this command:
\begin{lstlisting}
sudo raspi-config
\end{lstlisting}
\FloatBarrier
\begin{figure}[h]

       \centering

        \includegraphics[width=0.5\textwidth]{5v}
   
        \caption{Enable vnc menu}
        \label{fig:Enable vnc menu}

    \end{figure}
\FloatBarrier
Go to Interfacing options > vnc and click on “Select” While their, enable vnc
\item{Prepare working environment}\\
\FloatBarrier
\begin{figure}[h]

       \centering

        \includegraphics[width=0.5\textwidth]{prepareEnv}
   
        \caption{prepareEnv.sh file}
        \label{fig:prepareEnv.sh file}

    \end{figure}
\FloatBarrier
I have created \textbf{prepareEnv.sh} that execute the following:
\begin{itemize}
    \item  \commandbox*{mkdir project}:Create a new directory named  \commandbox{project} using the  \commandbox*{mkdir} command.
    \item  \commandbox*{cd project}:Change the current working directory to the \commandbox{project} directory using the "cd" command.
    \item  \commandbox*{git clone https://github.com/ultralytics/yolov5}:Clone the "yolov5" repository from the GitHub account of "ultralytics" using the  \commandbox*{git clone} command.
    \item  \commandbox*{sudo pip install virtualenv}:Install the "virtualenv" package using the \commandbox*{sudo pip install} command, which creates a virtual environment in the \commandbox{env} directory.
    \item  \commandbox*{cd yolov5}:Change the current working directory to the \commandbox{yolov5}directory using the \commandbox*{cd} command.
    \item  \commandbox*{. env/bin/activate}:Activate the virtual environment created earlier using the \commandbox*{.} command followed by the path to the \commandbox{env/bin/activate} file.
    \item  \commandbox*{pip install -r requirements.txt}:Install the Python packages listed in the "requirements.txt" file using the \commandbox*{pip install -r} command.
    \item  \commandbox*{pip3 install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1}:Install  torch version 1.13.1 torchvision version 0.14.1 torchaudio version 0.13.1 packages using the \commandbox*{pip3 install} command because the the ones in "requirements.txt" file are not compatible with the current yolov5.
    \item  \commandbox*{pip install gpiozero}:Install the \commandbox{gpiozero} package using the \commandbox*{pip install} command.
    \item  \commandbox*{pip install RPi.GPIO}:Install the \commandbox{RPi.GPIO} package using the \commandbox*{pip install} command.
    \item  \commandbox*{pip install subprocess.run}:Install the \commandbox*{subprocess.run} package using the \commandbox*{pip install} command.
    \item  \commandbox*{git clone https://github.com/AhmedOmrani10/YoloV5LogoDetection.git}:Clone the "YoloV5LogoDetection" repository from the GitHub account of "AhmedOmrani10" using the \commandbox*{git clone} command.
\end{itemize}
To execute the prepareEnv.sh just run this command:
\commandbox*{sh prepareEnv.sh}
This table represent the total time taking by the command:
\FloatBarrier
\begin{table}[h]

\centering
\tcbox[
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title = Execution time of prepareEnv.sh]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



\begin{tabular}{|c|c|}

       \hline
        \textbf{Metric} & \textbf{Value} \\
        \hline
        real & 18.759 m \\
        \hline
        user & 3.334 s \\
        \hline
        sys & 31.253 s \\
        \hline
\end{tabular}}
    \addcontentsline{lot}{table}{Execution time of prepareEnv.sh}

\label{tab:time}
\end{table}

\FloatBarrier
%\begin{table}[h]
   % \centering
    %\begin{tabular}{|c|c|}
      %  \hline
        %\textbf{Metric} & \textbf{Value} \\
        %\hline
        %real & 18.759 m \\
        %\hline
        %user & 3.334 s \\
       % \hline
        %sys & 31.253 s \\
        %\hline
    %\end{tabular}
    %\caption{Execution time of sh prepareEnv.sh}
    %\label{tab:time}
%\end{table}

\end{itemize}
\subsubsection{Deep learning pipeline}
\begin{itemize}
\item{Selecting the deep learning model: Why YOLOv5s?}//
Before delving into the reasons for choosing YOLOv5s, it is important to understand some of the key metrics used to evaluate object detection models. 
\begin{itemize}
\item Precision:
\begin{equation}
P = \frac{TP}{TP+FP}
\end{equation}
Precision, also known as positive predictive value, is a metric used in machine learning to evaluate the accuracy of a classification model. It is calculated as the ratio of true positives to the sum of true positives and false positives. When expressed as a probability, it represents the likelihood that a randomly selected instance classified as positive is actually a true positive. A perfect classifier with no false positives has a precision of 1.\cite{KGMM21}

\item Recall:

\begin{equation}
R = \frac{TP}{TP+FN}
\end{equation}
Recall, also known as sensitivity, is a performance metric used in machine learning that measures the proportion of positive instances that are correctly identified as positive by the model. When expressed as a probability, it represents the likelihood that a randomly selected true positive instance will be correctly classified as positive by the model. Unlike precision, which only takes into account instances predicted as positive by the model, recall considers all actual positive instances.\cite{KGMM21}

\item mAP:

\begin{equation}
mAP = \frac{1}{n} \sum_{k=1}^{N} AP_k
\end{equation}
mAP is a metric based on the area under precision-recall curve (PRC) that is preprocessed to eliminate zig-zag behavior (Padilla et al., 2021 [1]). Where $AP_k$ is the average precision (AP) of class $k$ and $n$ is the number of thresholds. The mAP values were calculated at $Z$ value threshold value for intersection over union (IoU) meaning, all the predicted bounding boxes that resulted in ratios of overlapping areas to the union areas with ground truth bounding greater than $Z$ were considered and the remaining were discarded.\cite{MG21}

\item IoU:
\begin{equation}
IoU(A,B) = \frac{A \cap B}{A \cup B}, \quad IoU(A,B) \in [0,1]
\end{equation}
Intersection over Union (IoU) is a commonly used method to evaluate object detection algorithms. It measures the overlap between the proposed bounding box and the ground truth bounding box by computing the ratio of their intersection over their union. If the resulting value is above a certain threshold, the proposed bounding box is considered a correct detection.\cite{MG21}
\end{itemize}
In Table~\ref{tab:yolov5_comparison} and the firgure \ref{fig:comparisation}, we compare the performance of different YOLOv5 models.
\FloatBarrier
\begin{figure}[h]

       \centering

        \includegraphics[width=0.7\textwidth]{comparisationYolov5}
   
        \caption{Comparison between yolov5 family and Efficientdet for the COCO dataset in term of speed and accuracy}
        \label{fig:comparisation}

    \end{figure}
\FloatBarrier
\begin{table}[h]

\centering
\tcbox[width=\linewidth,
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title = Comparison of YOLOv5 Models \cite{U23}]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



  
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Size (pixels)} & \textbf{mAPval@50-95} & \textbf{mAPval@50}  \\ \hline
    YOLOv5n & 640 & 28.0 & 45.7\\ \hline
    YOLOv5s & 640 & 37.4 & 56.8 \\ \hline
    YOLOv5m & 640 & 45.4 & 64.1 \\ \hline
    YOLOv5l & 640 & 49.0 & 67.3  \\ \hline
    YOLOv5x & 640 & 50.7 & 68.9  \\ \hline
  \end{tabular}}
    \addcontentsline{lot}{table}{Comparison of YOLOv5 Models part 1 \cite{U23}}

\label{tab:yolov5_comparison1}
\vspace{0.5cm}

\footnotesize All YOLOv5 checkpoints in the table are trained up to 300 epochs with default settings. The Nano and Small models use hyp.scratch-low.yaml hyps, while all other models use hyp.scratch-high.yaml. The mAPval values shown in the table are for single-model single-scale on the COCO val2017 dataset. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65".\cite{U23}

The speed measurements shown in the table are averaged over COCO val images using an AWS p3.2xlarge instance. The NMS times, which take about 1 ms per image, are not included in these measurements. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 640 --task speed --batch 1".\cite{U23}

The TTA (Test Time Augmentation) Test Time Augmentation includes reflection and scale augmentations. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 1536 --iou 0.7 --augment".\cite{U23}
\end{table}

\FloatBarrier
\FloatBarrier
\begin{table}[h]

\centering
\tcbox[width=\linewidth,
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title = Comparison of YOLOv5 Models part2 \cite{U23}]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



  
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Speed CPU b1 (ms)} & \textbf{Speed V100 b1 (ms)} & \textbf{Speed V100 b32 (ms)} & \textbf{Params (M)} \\ \hline
    YOLOv5n & 45 & 6.3 & 0.6 & 1.9 \\ \hline
    YOLOv5s &  98 & 6.4 & 0.9 & 7.2 \\ \hline
    YOLOv5m & 224 & 8.2 & 1.7 & 21.2 \\ \hline
    YOLOv5l & 430 & 10.1 & 2.7 & 46.5 \\ \hline
    YOLOv5x & 766 & 12.1 & 4.8 & 86.7 \\ \hline
  \end{tabular}}
    \addcontentsline{lot}{table}{Comparison of YOLOv5 Models \cite{U23}}

\label{tab:yolov5_comparison2}
\vspace{0.5cm}

\footnotesize All YOLOv5 checkpoints in the table are trained up to 300 epochs with default settings. The Nano and Small models use hyp.scratch-low.yaml hyps, while all other models use hyp.scratch-high.yaml. The mAPval values shown in the table are for single-model single-scale on the COCO val2017 dataset. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65".\cite{U23}

The speed measurements shown in the table are averaged over COCO val images using an AWS p3.2xlarge instance. The NMS times, which take about 1 ms per image, are not included in these measurements. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 640 --task speed --batch 1".\cite{U23}

The TTA (Test Time Augmentation) Test Time Augmentation includes reflection and scale augmentations. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 1536 --iou 0.7 --augment".\cite{U23}
\end{table}

\FloatBarrier
\FloatBarrier
%\begin{table}[h!]
  %\centering
  
 % \begin{tabular}{|c|c|c|c|c|c|c|c|}
    %\hline
    %\textbf{Model} & \textbf{Size (pixels)} & \textbf{mAPval@50-95} & \textbf{mAPval@50} & \textbf{Speed CPU b1 (ms)} & \textbf{Speed V100 b1 (ms)} & \textbf{Speed V100 b32 (ms)} & \textbf{Params (M)} \\ \hline
   % YOLOv5n & 640 & 28.0 & 45.7 & 45 & 6.3 & 0.6 & 1.9 \\ \hline
    %YOLOv5s & 640 & 37.4 & 56.8 & 98 & 6.4 & 0.9 & 7.2 \\ \hline
   % YOLOv5m & 640 & 45.4 & 64.1 & 224 & 8.2 & 1.7 & 21.2 \\ \hline
   % YOLOv5l & 640 & 49.0 & 67.3 & 430 & 10.1 & 2.7 & 46.5 \\ \hline
    %YOLOv5x & 640 & 50.7 & 68.9 & 766 & 12.1 & 4.8 & 86.7 \\ \hline
  %\end{tabular}
%\caption{Comparison of YOLOv5 Models \cite{U23}}
  %\label{tab:yolov5_comparison}
\vspace{0.5cm}

%\footnotesize All YOLOv5 checkpoints in the table are trained up to 300 epochs with default settings. The Nano and Small models use hyp.scratch-low.yaml hyps, while all other models use hyp.scratch-high.yaml. The mAPval values shown in the table are for single-model single-scale on the COCO val2017 dataset. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65".\cite{U23}

%The speed measurements shown in the table are averaged over COCO val images using an AWS p3.2xlarge instance. %The NMS times, which take about 1 ms per image, are not included in these measurements. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 640 --task speed --batch 1".\cite{U23}

%The TTA (Test Time Augmentation) Test Time Augmentation includes reflection and scale augmentations. To reproduce these results, one can run the command "python val.py --data coco.yaml --img 1536 --iou 0.7 --augment".\cite{U23}
%\end{table}
\FloatBarrier

The \textbf{YOLOv5} model is a state-of-the-art object detection model that has gained popularity due to its high accuracy and speed. The YOLOv5 family includes several models such as YOLOv5n, YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x, each with different sizes and performance characteristics. To determine the best model for our application of logo detection on a Raspberry Pi 4, we used those table above to compare the YOLOv5 models using metrics such as mAPval@50-95, mAPval@50, speed on CPU and V100, number of parameters, and FLOPs.//

Firstly, let's consider the Yolov5n model, which has the smallest size(1.9 million params) and computational requirements in the Yolov5 family. While this model is suitable for low-resource devices, it sacrifices accuracy (mAPval50-95 = 28.0 and mAPval50 =  45.7) for speed ( CPU b1 = 45 ms  , Speed V100 b1 = 6.3 ms and Speed V100 b32 = 	0.6 ms), making it less suitable for complex object detection tasks.

Next, the Yolov5m model strikes a balance between accuracy(mAPval50-95 = 45.4 and mAPval50 =  64.1) and speed ( CPU b1 = 224 ms, Speed V100 b1 = 8.2 ms and Speed V100 b32 = 1.7 ms), making it a popular choice for most applications. However, it still requires significant computational power (21.2 million params), which may not be feasible for low-resource devices like the Raspberry Pi 4.

The Yolov5l and x models offer the highest accuracy (mAPval50-95 = 49.0 and mAPval50 =  67.3 and mAPval50-95 = 50.7 and mAPval50 =  68.9 ) but come with even greater computational requirements (46.5 and 86.7 million params). They are best suited for high-end GPUs or cloud computing platforms and are not ideal for the Raspberry Pi 4.

Finally, we come to Yolov5s, which is the best choice for the Raspberry Pi 4. It has a smaller size and computational requirements (7.2 million params) than the other models in the family, making it easier to deploy on low-resource devices. Additionally, Yolov5s achieves high accuracy (mAPval50-95 = 37.4 and mAPval50 =  56.8) while maintaining a fast inference speed ( CPU b1 = 98 ms, Speed V100 b1 = 6.4 ms and Speed V100 b32 = 0.9 ms), making it an excellent choice for object detection tasks that require real-time performance.

In summary, the Yolov5s model is the best choice for the Raspberry Pi 4 due to its small size, low computational requirements, and high accuracy. It strikes the perfect balance between speed and accuracy, making it ideal for most object detection tasks.


\item{Building our dataset}
\FloatBarrier
\begin{figure}[ht]
  \centering
  \begin{minipage}[b]{0.45\linewidth}
    \includegraphics[width=\linewidth]{normalDataset}
    \caption{Normal dataset}
    \label{fig:image1}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\linewidth}
    \includegraphics[width=\linewidth]{invertedDataset}
    \caption{Inverted dataset}
    \label{fig:image2}
  \end{minipage}
\end{figure}
\FloatBarrier

To train our Yolov5s model, we first gathered a dataset of 726 images containing logos. We split the dataset into two classes: Normal and Inverted. The Normal class \ref{fig:image1} contained 363 images of logos in their standard orientation, while the Inverted class \ref{fig:image2} contained 363 images of logos that had been rotated 180 degrees.

\item{Image labeling}
\FloatBarrier
\begin{figure}[ht]
  \centering
    \includegraphics[width=\linewidth]{labeling}
    \caption{Labeling an image}
    \label{fig:labeling}


\end{figure}
\FloatBarrier
I used Roboflow to perform the image labeling. Image labeling involves annotating the images with bounding boxes that define the location of the logos in each image. This step is essential for training an object detection model like Yolov5s.
After hours of scouring through an endless stream of images and yelling at my computer screen, I finally finished the task of labeling the images for our dataset. With every annotation made, I felt as if a weight had been lifted from my shoulders (and added to my mouse finger). The satisfaction of seeing all of our hard work come together was almost as great as the relief of knowing I won't have to do it again anytime soon. Now, with our dataset fully labeled and ready for training, it's time to sit back, relax, and let the computer do the heavy lifting.
\item{Image data augmentation}
\FloatBarrier
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{augmentatedDataset}
    \caption{Augmented dataset}
    \label{fig:Augmented dataset}
\end{figure}
\FloatBarrier
For data augmentation, we used several techniques in Roboflow, including Hue, Saturation, Brightness, and Blur. Hue refers to the color of the image, and we applied random rotations between -11° and +11° to create variations in color. Saturation refers to the intensity of the colors, and we applied random variations between -14\% and +14\% to increase the diversity of the dataset. Brightness refers to the overall brightness of the image, and we applied random variations between 0\% and +36\% to simulate different lighting conditions. Finally, we applied a blur of up to 0.75 pixels to simulate blurring caused by motion or other factors.

After augmenting our dataset, we ended up with triple the number of images, which allowed us to train a more robust model.


\item{Data loading}
\begin{lstlisting}[language=Python]
# Import the Roboflow package
from roboflow import Roboflow

# Instantiate a Roboflow object and provide your API key
rf = Roboflow(api_key="AKSDKS85skldkMLD755S")

# Specify the project and dataset that you want to download
project = rf.workspace("ahmed-omrani-uqhfw").project("logodetection-fmddc")
dataset = project.version(3).download("yolov5")


\end{lstlisting}
The training data was downloaded using the Roboflow Python package, which is a popular tool for managing and preprocessing computer vision datasets. The training data was downloaded from a specific Roboflow project and was in the YOLOv5 PyTorch format
\item{Transfer learining on yolov5s}\\
Fine-tuning is a popular technique in machine learning that involves taking a pre-trained model and adapting it to a new problem. Transfer learning is the process of using a pre-trained model to jumpstart a new model's training. These techniques are widely used in computer vision tasks where labeled data is limited, and collecting more data is costly. By fine-tuning a pre-trained model, the model can leverage its knowledge and adapt to the new task quickly. The process typically involves freezing some of the pre-trained layers and only updating the weights of the output layer or a few top layers. By doing so, the model can adapt its weights to the new problem while retaining its knowledge of the previously learned features. This approach often leads to improved performance with fewer labeled examples.
\FloatBarrier
\begin{lstlisting}[language=Python]
  %%writetemplate /content/yolov5/models/custom_yolov5s.yaml

# parameters
nc: {num_classes}  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 3, C3, [1024]],
   [-1, 1, SPPF, [1024, 5]],  # 9
  ]

# YOLOv5 v6.0 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)

   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]

\end{lstlisting}
\FloatBarrier
In this case and as you can see from the code above , the YOLOv5s model was fine-tuned to detect two classes: normal logos and inverted logos, instead of the original model which was trained on 80 classes. The model was modified to recognize only these two specific classes by adjusting the number of classes parameter in the YOLOv5s model's configuration file. The configuration file, which is a YAML file that specifies the network architecture and its parameters, was updated to reflect the change in the number of classes. This customization allows the model to be optimized for the specific task of detecting normal and inverted logos, rather than having to account for the nuances of 80 different classes.
By using transfer learning, only the last layer of the YOLOv5s model was modified to detect the two new classes. The other layers of the model were left unchanged, as they were already trained to detect objects. This means that the feature extraction layers of the model are kept intact and only the classification layers are modified.
The modified configuration file was saved as custom\_yolov5s.yaml. This file was used to train the fine-tuned YOLOv5s model.
\item{Training the fine-tuned yolovs}

Once the data was downloaded, the training process was initiate. The model was trained using the train.py script from the YOLOv5 repository. The script was configured to use the custom configuration file and the downloaded training data.
To validate the YOLOv5 model, we use the following line of code:
\begin{lstlisting}[language=Python]
!python train.py --img 416 --batch 16 --epochs 300 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml  --name yolov5s_results  --cache
\end{lstlisting}
During the training process, the weights of the YOLOv5s model were updated to detect only two classes. The model was trained for 300 epochs, with a batch size of 16 and an image size of 416x416. The model was also cached, which means that the weights of the model were saved after each epoch, reducing the training time in the future.

\FloatBarrier
\begin{table}[h]

\centering
\tcbox[
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title =Training and Validation Metrics for YOLOv5s]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



\begin{tabular}{|c|c|c|c|}
\hline
 \textbf{Precision} & \textbf{Recall} & \textbf{mAP50} & \textbf{mAP50-95} \\ \hline
 0.998 & 1.000 & 0.995 & 0.889 \\ \hline
\end{tabular}}
    \addcontentsline{lot}{table}{Training and Validation Metrics for YOLOv5s}


\end{table}

\FloatBarrier


The training output shows that the model achieves a precision of 0.999, a recall of 1, mAP50 of 0.995, and mAP50-95 of 0.889. These metrics indicate that the model has learned to detect objects with high accuracy.

\item{Validating the fine-tuned yolovs}
Validation is a critical step in the training process, which involves evaluating the performance of the trained model on a validation dataset. The validation dataset is a separate dataset from the training dataset that is used to ensure that the model is not overfitting to the training data.

To validate the YOLOv5 model, we use the following  line of code :
\begin{lstlisting}[language=Python]
!python val.py --weight "runs/train/yolov5s_results/weights/best.pt" --data "/content/yolov5/LogoDetection-3/data.yaml"
\end{lstlisting}
The val.py script loads the trained model and evaluates it on the validation dataset specified in the data.yaml file. The --weight flag specifies the path to the weights of the trained model, and the --data flag specifies the path to the dataset configuration file.
\FloatBarrier

\begin{table}[h]

\centering
\tcbox[
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title =Training and Validation Metrics for YOLOv5s]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



\begin{tabular}{|c|c|c|c|}
\hline
 \textbf{Precision} & \textbf{Recall} & \textbf{mAP50} & \textbf{mAP50-95} \\ \hline
0.999 & 1.000 & 0.955 & 0.877 \\ \hline
\end{tabular}}
    \addcontentsline{lot}{table}{Training and Validation Metrics for YOLOv5s}


\end{table}

\FloatBarrier

The validation output shows that the model achieves a precision of 0.99, a recall of 1, mAP50 of 0.955, and mAP50-95 of 0.877. These metrics indicate that the model is performing well on the validation dataset and didn't overfit that much to the training data.

\item{Testing the fine-tuned yolovs}
Testing is the process of evaluating the performance of the trained model on a completely new dataset that the model has never seen before. It is used to ensure that the model can generalize well to new data.
To test the YOLOv5 model, we use the following line of code:
\begin{lstlisting}[language=Python]
!python val.py --weight "runs/train/yolov5s_results/weights/best.pt" --data "/content/yolov5/LogoDetection-3/data.yaml" --task test
\end{lstlisting}
This command is used to evaluate the performance of the trained YOLOv5 model on a test dataset. The val.py script is used here again, but this time with the --task test flag which tells the script to evaluate the model on the test dataset. The --weight flag specifies the path to the best weights file obtained during training. The --data flag specifies the path to the dataset YAML file containing information about the test dataset. 
\FloatBarrier

\begin{table}[h]

\centering
\tcbox[
  left=0mm,
  right=0mm,
  top=0mm,
  bottom=0mm,
  boxsep=0mm,
  toptitle=0.5mm,
  bottomtitle=0.5mm, title =Training and Validation Metrics for YOLOv5s]{%
\arrayrulecolor{blue!50!black}\renewcommand{\arraystretch}{1.2}%



\begin{tabular}{|c|c|c|c|}
\hline
 \textbf{Precision} & \textbf{Recall} & \textbf{mAP50} & \textbf{mAP50-95} \\ \hline
0.999 & 0.997 & 0.954 & 0.878 \\ \hline
\end{tabular}}
    \addcontentsline{lot}{table}{Training and Validation Metrics for YOLOv5s}


\end{table}

\FloatBarrier
The test output shows that the model achieves a precision of 0.999, a recall of 0.977, mAP50 of 0.994, and mAP50-95 of 0.878. These metrics indicate that the model is performing well on the test and didn't overfit that much to the training data.

\item{Classes Detection}\\
After completing the crucial steps of image gathering, image labeling, image augmentation, and training the YOLOv5 model, we finally reach the intense step of inference. Inference is the process of using the trained model to make predictions on new, unseen images. This step is the culmination of all the hard work put into building and refining the model, and it is where the model's true effectiveness is put to the real-life test. Through inference, we can evaluate the model's accuracy and see how well it performs on real-world data, ultimately determining its usefulness in solving the problem at hand.\\
During inference, the YOLOv5 algorithm takes an input image and passes it through a convolutional neural network (CNN) to extract feature maps. The CNN uses several layers of convolution and pooling operations to extract features from the image. These features are then passed through a series of fully connected layers to produce a set of bounding boxes and associated confidence scores for each detected object.

The bounding boxes are defined by their coordinates (x, y) and their width and height (w, h). The confidence scores indicate the probability that an object is present in the bounding box. During inference, the YOLOv5 algorithm evaluates the confidence scores for each detected object and selects the object with the highest score as the primary detection.

\FloatBarrier

\begin{figure}[htbp]
    \centering
    
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{normal1}
        \caption{Normal class detection}
        \label{fig:image1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{inverted1}
        \caption{Inverted class detection}
        \label{fig:image2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{nothing1}
        \caption{Detection of nothing}
        \label{fig:image3}
    \end{minipage}

    
\end{figure}
\FloatBarrier
As we can see in figure \ref{fig:image1} If an object is in an inverted position, the YOLOv5 algorithm will still be able to detect it. This is because the CNN is able to learn and recognize patterns and features of objects, regardless of their orientation in the image. The algorithm will simply detect the object and output its bounding box and confidence score, regardless of whether it is in a normal  in figure \ref{fig:image2}  or inverted position  in figure \ref{fig:image1} and if it doesn't we can coclude that there is no logo  in figure \ref{fig:image3}.
\begin{figure}[htbp]

    

        \centering
        \includegraphics[width=\textwidth]{leftside1}
        \caption{Logo on the left side detection}
        \label{fig:image4}
\end{figure}
Now To detect if the logo is printed on the right or left side of the product, you can use image processing techniques. First, you can extract the coordinates of the bounding box resulting from the YOLOv5 prediction of the nromal class as shown in the figure \ref{fig:image4} by assigning xmin, ymin, xmax, ymax to the variables row[0], row[1], row[2], and row[3], respectively.
\begin{lstlisting}[language=Python]
xmin, xmax = row[0],row[2]
\end{lstlisting}
Next, you can calculate the center of the bounding box by finding the average of xmin and xmax. This can be done with the following code:
\begin{lstlisting}[language=Python]
center_x = (xmin + xmax) / 2
\end{lstlisting}
Then, you can calculate the width of the frame using the shape attribute of the frame, which returns the height, width, and number of channels of the image. This can be done with the following code:
\begin{lstlisting}[language=Python]
width = frame.shape[1]
\end{lstlisting}
the code above  calculates the width of the image using the shape attribute of the frame, but only assigns the second element (index 1) to the variable width, which corresponds to the width of the image. The height and number of channels are not used in this code.
Finally, you can compare the center of the bounding box with the center of the image. If the center of the bounding box is smaller than the center of the image, it means that the logo is located on the left side of the product, which is a defective piece. If it is on the right side, it means that the logo is printed on the right side of the product, which is a normal piece.
\end{itemize}

\subsubsection{Relay module desgin}

\FloatBarrier
\begin{figure}[h]
\FloatBarrier
         \centering
        \includegraphics[width=1\textwidth]{relay}
   
        \caption{Schematic Capture of the relay}
        \label{Schematic Capture of the relay}
\FloatBarrier
    \end{figure}
\FloatBarrier
When interfacing a Raspberry Pi, which operates at 3.3V logic levels, with a device that requires a higher voltage, such as 24V in my case, using a relay becomes essential. The 3.3V GPIO signal from the Raspberry Pi may not be strong enough to directly trigger a response from the target device, like the automaton in your setup.

A relay acts as an electrical switch that can control higher voltages and currents using a lower voltage control signal. In this scenario, the 3.3V signal from the Raspberry Pi activates the 5V relay's coil, which then switches its contacts to allow the passage of the higher voltage signal (in this case, 24V) to the automaton.

By utilizing the relay, you create an isolation barrier between the low voltage control circuit (Raspberry Pi) and the high voltage load circuit (automaton). This isolation protects the Raspberry Pi's delicate electronics from potential damage caused by the higher voltage, ensuring safe and reliable operation.


In my project, I designed a PCB relay module using Proteus software. I started by creating a new project and selecting the schematic capture option. In the schematic editor, I added all the necessary components such as the relay, terminal blocks, and supporting elements like diodes or resistors. I carefully connected the components together, making sure the signals flowed correctly by drawing wires between their pins. To ensure clarity, I included labels and annotations.

Once the schematic was complete, I performed a design rule check to catch any errors or violations. I made the necessary revisions to ensure the schematic was error-free and ready for the next step.


\FloatBarrier
\FloatBarrier
\begin{figure}[h]
\FloatBarrier
         \centering
        \includegraphics[width=1\textwidth]{V3}
   
        \caption{Schematic Capture of the relay}
        \label{Schematic Capture of the relay}
\FloatBarrier
    \end{figure}
\FloatBarrier
Moving on to the PCB layout stage, I selected the PCB layout option in Proteus and imported the schematic. This generated an initial component placement, which I organized and optimized for efficient routing. I took into consideration factors like signal paths, component clearance, and the overall size of the PCB.

With the component placement finalized, I began routing the connections using Proteus's routing tools. I created traces that connected the appropriate pins of the components, following design rules and guidelines. I paid extra attention to critical paths and sensitive signals, ensuring proper separation and avoiding noise issues.

\section{Conclusion}
In conclusion, implementing YOLOv5 proved superior to contour detection in detecting the four different states of the logo: normal, inverted, absent, and on the left side. However, new challenges emerged that need to be addressed in the next chapter such as new defective states of the logo , network improvements and more.
%\input{biblio.tex}




%\end{document}